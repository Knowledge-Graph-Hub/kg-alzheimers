# Title
Cognitive reserve in ageing and Alzheimer's disease

# Abstract
The concept of reserve accounts for individual differences in susceptibility to age-related brain changes or Alzheimer's disease-related pathology. There is evidence that some people can tolerate more of these changes than others and still maintain function. Epidemiologic studies suggest that lifetime exposures including educational and occupational attainment, and leisure activities in late life, can increase this reserve. For example, there is a reduced risk of developing Alzheimer's disease in individuals with higher educational or occupational attainment. It is convenient to think of two types of reserve: brain reserve, which refers to actual differences in the brain itself that may increase tolerance of pathology, and cognitive reserve. Cognitive reserve refers to individual differences in how tasks are performed that may allow some people to be more resilient than others. The concept of cognitive reserve holds out the promise of interventions that could slow cognitive aging or reduce the risk of dementia.

## Cognitive and Brain Reserve
The concept of reserve has been put forward to account for individual differences in susceptibility to age-related brain changes and pathologic changes such as those that occur in Alzheimer's disease. Reserve can act as a moderator between pathology and clinical outcome, thus accounting for this discontinuity. One convenient, although somewhat artificial, differential classification of reserve is brain reserve versus CR.

The original concept of brain reserve was quantitative, for example more neurons or synapses to lose. This idea is supported by a set of studies that suggest that prevalence or incidence of dementia is lower in individuals with larger brains 13 , 14 . I have suggested that this is a passive model of reserve, in that it suggests that the brain can simply tolerate more pathology before it reaches a critical threshold for clinical symptoms to appear.

In contrast, the concept of CR suggests that the brain actively attempts to cope with brain damage by using pre-existing cognitive processing approaches or by enlisting compensatory approaches 15 , 16 . This would allow an individual with high CR to better cope with the brain damage than an individual with lower CR. In CR, brain function rather than brain size is the relevant variable. Thus the CR concept is an active form of reserve in that, the same amount of brain damage or pathology will have different effects on different people, even when brain size is held constant.

Although the initial conception of brain reserve was entirely quantitative, recent evidence suggests that this concept is more nuanced. For example stimulating environments foster the growth of new neurons in the form of neurogenesis 17 – 19 , and upregulate BDNF 20 , which fosters neural plasticity. Still, while in some ways interdependent, brain reserve and CR make independent in addition to synergistic contributions to understanding individual differences in clinical resilience to brain pathology. It is still an unresolved issue whether and how these two components of reserve interact.

Reserve was initially posited as a moderator between brain change and clinical outcome, but there are recent suggestions that life experience may also act to prevent or minimize pathology. On a simple level, it has always been recognized that exercise may serve to help prevent vascular disease. However, there are suggestions that cognitively stimulating activities may slow the rate of hippocampal atrophy in normal aging 21 , and perhaps even prevent accumulation of amyloid plaque 22 . While these ideas are promising and intriguing, they are beyond the scope of the current review, which will be limited to how CR may help cope with brain changes once they develop.

## Epidemiologic evidence for reserve
Our group first addressed the concept of CR in a study of incident dementia. The assumption is that disease pathology slowly develops over time independently of CR, and that the pathology begins many years before the onset of clinically diagnosed AD. Since people with greater reserve should be able to tolerate more AD pathology, the onset of clinical dementia should be delayed. We analyzed data from 593 non-demented individuals aged 60 years or older followed over 4 years 23 . Individuals with less than 8 years of education had 2.2 times higher risk of developing dementia compared to those with more education. We also examined the potential impact of occupational attainment. Based on US census categories, participants were grouped into low (unskilled/semiskilled, skilled trade or craft, and clerical/office worker) and high (manager business/government and professional/technical) occupational levels. Those with low lifetime occupational attainment also had 2.25 times greater risk of developing dementia than those with higher lifetime occupational attainment. The implication of these findings was that educational and occupational experiences imparted a reserve against the expression of Alzheimer's pathology. In a later study, we assessed participation in a variety of leisure activities in another set of nondemented elders 24 . An interview elicited self-reported participation during the month preceding the interview in the following 13 activities: knitting or music or other hobby, walking for pleasure or excursion, visiting friends or relatives, being visited by relatives or friends, physical conditioning, going to movies or restaurants or sporting events, reading magazines or newspapers or books, watching television or listening to the radio, doing unpaid community volunteer work, playing cards or games or bingo, going to a club or center, going to classes, and going to church or synagogue or temple. We divided the participants with low (<=6) or high (>6) participation in leisure activities.. Those who engaged in more leisure activities had 38% less risk of developing dementia. A review paper 25 found 22 papers reporting cohort studies of the effects of education, occupation, premorbid IQ and mental activities in incident dementia published up to 2004. The great majority of the studies demonstrated a significant protective effect of these lifetime exposures. The authors summarized all of the studies to calculate the protective effect of higher CR and found that it decreased the risk of developing dementia by 46%.

In contrast, once AD emerges, those with higher reserve show more rapid decline. In one of the earliest studies of this type, we matched patients with AD for clinical severity and then followed them over time 26 . Those with greater education or occupational attainment died sooner than those with less attainment. In a subsequent analysis, we also found that those with higher educational or occupational attainment (as defined above) had more rapid cognitive decline 27 . On the average scores on a memory test decline by about one point yearly, but in those with higher educational or occupational attainment scores decline by an additional point each year. We replicated this observation in patients with incident AD 28 and also found more rapid decline in cognitive function in AD patients who engaged in more leisure activities prior to dementia onset 29 .

Our theoretical explanation for these findings is illustrated in figure 1 . Individuals with higher CR can tolerate more pathology so the point at which cognitive functions begin to be affected will be later than in those with lower CR. However, we reasoned that there is common point in all people where the pathology is so severe that function cannot be maintained. Given these assumptions, individuals with higher CR will begin their cognitive decline when pathology is more advanced and thus have less time until they reach the point where pathology overwhelms function. This results in a more rapid rate of decline once it begins.

While these epidemiologic studies are supportive of the concept of cognitive reserve, they cannot be considered definitive evidence. Only controlled studies can truly establish that some set of interventions or experiences are beneficial. This is discussed more fully below.

## Neuroimaging Studies of Cognitive Reserve
The epidemiologic studies suggest that at any given level of clinical AD severity an individual with a higher level of CR should have greater AD pathology. This idea is illustrated in figure 2 . We tested this idea using resting regional cerebral blood flow (rCBF) as a surrogate for AD pathology 30 , 31 . In AD patients matched for clinical severity, we found an inverse relationship between resting rCBF and years of education 32 . Higher education was associated with more depleted flow in the parietotemporal area, the location of PET changes in AD. These results are illustrated in figure 3 (from {Stern, 1992 #4423}). This observation provided some initial indication that patients with higher CR can tolerate more AD pathology than those with lower CR and still appear clinically similar. Similar analysis demonstrated a comparable protective effect of occupational attainment 33 and leisure activities 34 . These observations using cerebral blood flow have been replicated by other groups as well 35 , 36 . Later, these observations were replicated in a postmortem study. The investigators looked at the brains of 130 elders who underwent cognitive evaluation and then came to autopsy. They found that both education and a summary measure of Alzheimer's disease pathology were associated with cognitive performance prior to death. In addition, they found that education modified the association between AD pathology and levels of cognitive function measured prior to death. For each additional year of education, the relationship between pathology and cognition was reduced by 0.088 standard units. Thus, at any given level of brain pathology, higher education was associated with better cognitive function 37 .

The reviewed studies suggest that life exposures such as education, occupation, and leisure activity can provide reserve against age-or AD related pathology. However, the cognitive or neural mechanism that underlies this reserve is unknown. Our group has focused on using functional imaging to identify networks that might mediate CR. The neural implementation of CR might take two forms: neural reserve and neural compensation 16 , 38 . The former addresses the idea that CR could be associated with individual differences in the resilience of pre-existing cognitive networks. The latter suggests that some individuals may be able to recruit compensatory resources better than others.

The key idea behind neural reserve is that CR might be mediated by the same networks that are utilized by individuals in the absence of age-or disease-related pathology. For example, the differential efficiency or capacity of these networks may account for individual differences in performance as well as individual differences in the ability to cope with brain change. In an fMRI study, we examined the issue of network efficiency and capacity in young and old adults 39 . We manipulated the extrinsic difficulty of a task by systematically varying the response deadline. Using a covariance method to analyze the imaging data, we identified a spatial pattern of fMRI activation that was expressed by both groups and whose expression increased as the task became more difficult. This pattern was expressed to a greater degree by the old group than the young group when the task was easiest, consistent with the idea that the older group had reduced network efficiency. In contrast, the pattern was expressed to greater degree by the younger group when the task was hardest, consistent with the idea that at the highest demand the younger participants performed better than the elders because of greater network capacity. The differences noted in efficiency and capacity across the two age groups are also present across individuals within each age group. It is possible that individuals with more efficient or higher capacity networks could have more resilience in the face of age- or disease-related changes.

A series of imaging studies also explored the issue of neural reserve and neural compensation. Young and older healthy adults were assessed with the letter Sternberg task. In this task, subjects study one, three, or six letters for 3 seconds (stimulus phase) followed by a 7 second retention phase. They are then presented with a single probe letter and are asked to indicate whether that letter is a member of the previously studied set. For our fMRI analyses, we were most interested in aspects of activation that change as a function of the increase in the number of letters that are studied -- what we term load-related activation. Using a covariance analytic approach (multivariate linear modeling, MLM) we found that load-related activation during the retention phase of the task was characterized by two fMRI networks 40 . Each of these networks is a group of brain areas that seem to work together as the task is harder. This is inferred from the observation that there the change in fMRI signal as the task gets harder is correlated across these areas. The first network was used by both young and old subjects, and consisted of areas often associated with working memory. In contrast, the second network was primarily characterized by activation in parahippocampal areas and was used consistently only by the older subjects. Importantly, the older subjects who used this second pattern more performed worse on the task. In a follow-up analysis 41 , we found that atrophy in a key area within the first network (i.e. the one that was used by both young and old) was associated with decreased efficiency in that network and increased utilization of the second network. This observation is consistent with the idea that as age-related changes limit the efficiency of the first network, elders increasingly recruited the alternate network. Those who rely more on this alternate network can still perform the task, but do so more poorly. This finding is an example of neural compensation.

Another analysis 42 evaluated the role of CR in the findings just described and found two distinct influences of CR (as measured by IQ). First, individuals with higher CR could tolerate more atrophy in the first network and still preserve that network’s performance without having to resort to using the second network. Second, compared to individuals with lower CR, those with higher CR performed better even when they use the second network.

The latter finding suggests CR can make use of cognitive resources that are separate from those directly involved with task performance. This is consistent with the idea that there may be some generalized neural representation of CR that might impart protection across a wide range of tasks. In order to investigate this possibility, we analyzed imaging data from individuals performing two different tasks with varying demands. 43 In a young group of subjects we identified an activation network that was expressed during the stimulus presentation phase of both tasks and whose increase in expression with increasing load correlated with CR. This observed network is consistent with a neural instantiation of CR. In the future, identifying a neural pattern of activation that is associated with generic CR would provide a direct way of measuring any individual’s level of reserve. Further, a quantifiable neural CR pattern would provide a useful stratification or outcome measure for pharmacologic or non-pharmacologic studies intended to improve cognitive functioning.

## Application Of Cognitive Reserve In Clinical Practice
This section provides some considerations for the application of CR in clinical practice. When assessing cognition as part of a diagnostic evaluation, it is important to take into account the most appropriate and valid indicator of CR for a given patient. In the event that an individual’s level of education is not believed to be a good representation of his or her optimal cognitive functioning, assessment of IQ or consideration of occupation may provide a more accurate estimate.

Integration of the most appropriate and valid measure of CR into the diagnostic formulation is critical. Individuals with high reserve, by definition, will not demonstrate disease-related clinical symptoms as early as individuals with low levels of reserve. This issue might partially be addressed with instrumentation, such that: 1) more challenging tests with higher ceilings may better detect changes in individuals with very high levels of functioning, 2) tests that are more pathologically-specific (e.g., associative learning tasks for the hippocampus) may have greater sensitivity in high reserve individuals. However, clinicians must be aware that in the presence of pathology there should still be a period of time during which even the most sensitive measures would fail to detect cognitive change.

Information regarding brain integrity could be be integrated with cognitive data for diagnostic purposes. Neuroimaging tools have the potential, particularly in individuals with high reserve who maintain cognitive functioning for an extended period of time, to detect pathological changes when impairment on neuropsychological testing is subtle. For example, at a given level of clinical severity, AD patients with higher education have a more severe pattern of AD-related changes on PET scan than those with lower education 44 , 45 . Thus, neuroimaging could be an aid in effective diagnosis. Integration of this concept into day-to-day practice is premature, however. Today it is possible to detect the presence of amyloid in the brain with either PET imaging or cerebrospinal fluid testing, but the prognostic implications of these tests is not fully established. Still, the availability of these biomarkers provides a fruitful avenue for research into cognitive reserve.

CR should also be recognized as a factor that will influence rate of cognitive decline following diagnosis. The rate of decline is more rapid in individuals with high reserve than those with low reserve, even when accounting for a multitude of other factors that may contribute to disease course 27 , 28 , 46 . This has direct relevance for assessing the effectiveness of treatment. Given the intended mechanism of a particular medication, it may have divergent effects as a function of the degree of underlying pathology. Also, most clinical trials are designed to compare rate of decline in a treated and placebo group. A mismatch in CR across these two groups could lead to differential rates of decline that have nothing to do with the medication under study.

Finally, epidemiologic evidence linking certain life experiences and individual characteristics to lower rates of dementia, is not sufficient to determine definitively what experiences directly prevent or delay dementia. Intervention studies are needed to firmly establish causal links between life experiences, individual characteristics, and CR, and such studies are underway. Therefore, while recommending that patients engage in certain activities such as mental enrichment and physical fitness is likely not to be harmful and may in fact have numerous positive effects, clinicians should be careful not to present these activities as established treatments or fully proven preventative strategies against dementia.

## Implications of Cognitive Reserve for Remediation and Prevention
The epidemiologic evidence suggests that experiences at all stages, even in late life, can impart such reserve. These findings support the possibility that it may be possible to intervene even later in life in order to impart reserve, slow age-related cognitive decline, and prolonged healthy aging. The most successful remediation approach to date has been aerobic exercise. Many controlled studies in elderly individuals have demonstrated that, in individuals with below median respiratory capacity, aerobic exercise that increases this respiratory capacity also results in increases in cognitive performance 47 , 48 . The results of cognitive intervention studies have been more mixed. In one of the largest such studies there was no evidence that training in one cognitive domain generalized to performance in two other domains. In addition, there is no convincing evidence that training resulted in any improvement in activities of daily living 49 . More promising have been studies that have immersed subjects in complex gameplay. For example, Basak et al. 50 had elders play a complex role-playing game, "Rise of Nations" for 23.5 hours, and demonstrated that gameplay was associated with improved performance on a wide range of cognitive tasks. Similarly, focused training has improved working memory capacity and shown transfer of training to nontrained working memory tasks 51 . In schizophrenia, small effects of cognitive remediation were noted, particularly in stable patients and when combined with adjunctive rehabilitation 52 . Similar conclusions about the limitations and promise of cognitive intervention were reached by a recent working group 53 .

Our group has been working with another complex game, "Space Fortress," that was designed by human operations psychologists as a test bed for training approaches to teach young adults complex tasks 54 . In research with young adults, one particular training approach produced excellent improvement in gameplay, but, more importantly, also showed transfer of training to other tasks. In this approach, which is called emphasis change training, players are instructed to focus on all of the features of the game, but to give specific emphasis to one particular feature during each individual game. By shifting emphasis from game to game, participants cannot fall into a fixed strategy. They must develop the ability to deal with the entire task as a whole – skills that might be termed attentional allocation or executive control. Younger people who learn to play the game using this training approach were more able to incorporate new demands while playing the game 55 . In addition, they showed more successful performance on real-world tasks such as flight simulator or flight training performance 56 , 57 . In a preliminary study with older adults, we explored the capacity of the elders to learn to play the game, and the possibility that gameplay might improve cognitive performance 58 . 90 subjects were divided into three groups: a non-gameplay condition, gameplay without emphasis change training, and gameplay with emphasis change training. The two gameplay groups played the game three times a week for 12 weeks. Interestingly, although the game itself was very challenging for the elders, there were few dropouts and performance improved over time. Analysis of various features of gameplay indicated that elders in the pure gameplay condition were not as focused on the key goals of the game as the emphasis change group; they were more likely to respond to signals for bonus points as opposed to attempt to destroy the space fortress. In contrast, the elders in the emphasis change condition were more likely to focus on features central to the game, in particular they were eventually more successful at destroying the space fortress than elders and the other group. Our primary cognitive measures were five tasks that involve executive control. On one of these tasks, a test of working memory, the emphasis change group showed greater improvement from pre- to post-testing than the other two groups. We consider this a promising finding, and are currently conducting a study which combines space fortress gameplay with aerobic exercise. By combining a cognitive intervention with aerobic exercise we are hoping for a synergistic effect. The aerobic exercise may in effect boost brain reserve, for example by improving plasticity via upregulation of BDNF. The cognitive intervention, in turn, may help increase CR by staying the efficiency of the cognitive networks underlying executive control. This cognitive data fit would be facilitated by the improved level of brain reserve.

More generally, the epidemiologic data clearly suggest that aspects of life experience can impart reserve against age- or disease-related pathology. However, the exact "recipe" for helping to create this reserve is still unknown. A generic recommendation would be to maintain educational and mentally stimulating activities throughout life 59 . Specific recommendations await more focused, large-scale research interventions. The most meaningful endpoints for such an intervention would be slowed rate of cognitive decline in aging or reduced risk of developing Alzheimer's disease. No study to date has shown change in either of these endpoints. Optimally they would begin with cognitively intact elders, and follow them over a suitable period of time. Most likely, these studies should use multiple intervention strategies including exercise, cognitive stimulation and social stimulation. Studies using these endpoints will be very expensive because they will require a large number of participants and will have to be conducted over several years.

## Conclusion
The concept of CR first emerged from epidemiologic observations. While epidemiologic studies can only describe correlations and do not directly test causation, they are useful for deriving hypotheses and exploring ideas. A wealth of epidemiologic studies suggest that an array of life exposures seem to be associated with a reserve against age or AD-related pathology. While the original observations concern easily measurable variables like education or occupational attainment, more recent studies have delved more deeply into lifestyle factors, cognitively stimulating behaviors, personality factors and the like. In the aggregate, they suggest that contributions to reserve come from multiple sources, and that reserve is not a fixed entity but can change across the lifespan depending on exposures and behaviors. This is a hopeful idea, since it suggests that changes in lifestyle even later in life might impart reserve against age-related cognitive decline or dementia. Despite this exciting possibility, carefully controlled studies will be required in order to translate this idea into a practical intervention. Such studies would yield practical information both about the combination and timing of activities that may lead to more successful aging.

The desire to understand the neural basis of CR has been a motivating factor for functional imaging studies that may contribute to our understanding of the brain-behavior changes that occur with aging. Insight into the neural basis of cognitive reserve could produce information that would allow interventions to be better focused. Direct neural measures of cognitive reserve, perhaps based on patterns derived from fMRI studies, could be very useful. Consideration of the concept of CR can be useful in the clinical setting. From a practical point of view, consideration of cognitive reserve can be integrated into a practitioner’s diagnostic formulation. The fact that higher CR is associated with more rapid decline in patients with Alzheimer's disease makes it important to assure that CR is considered in clinical trials which rely on differences in the rate of decline between patients on drug and placebo. In this regard, directly measurable neural correlate of reserve, such as an fMRI pattern, would be very useful. More generally, consideration of cognitive and brain reserve moves us to a more systems-based approach to understanding conditions associated with brain changes. Since the brain both passively and actively attempts to cope with brain changes or pathology, it is important to understand the factors that contribute to this resilience.