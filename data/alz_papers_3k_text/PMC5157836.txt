# Title
Next-generation genotype imputation service and methods

# Abstract
Genotype imputation is a key component of genetic association studies, where it increases power, facilitates meta-analysis, and aids interpretation of signals. Genotype imputation is computationally demanding and, with current tools, typically requires access to a high-performance computing cluster and to a reference panel of sequenced genomes. Here we describe improvements to imputation machinery that reduce computational requirements by more than an order of magnitude with no loss of accuracy in comparison to standard imputation tools. We also describe a new web-based service for imputation that facilitates access to new reference panels and greatly improves user experience and productivity.

## ONLINE METHODS
We simulated haplotypes for a three-population coalescent model using the program ms 8 . We chose a demographic model consistent with patterns of diversity observed in European-ancestry samples 28 .

Here we describe the state space reduction that uses the similarity between haplotypes in small genomic segments to reduce computational complexity. We recommend first reading a description of the original minimac algorithm 9 . Consider a reference panel with H haplotypes and a genomic segment bounded by markers P and Q . Let U ≤ H be the number of distinct haplotypes in the block.

Label the original haplotypes as X 1 , X 2 , ..., X H and the distinct unique haplotypes as Y 1 , Y 2 , ..., Y U . For example, in Figure 1 , block B bounded by markers P = 1 and Q = 6 has U = 3 distinct haplotypes. Let L k ( . ) and L k ( . ) denote the left probabilities for the original states and reduced states at marker k (ref. 29 ). Assuming we know L P ( X 1 ), ..., L P ( X H ), equation (1) allows us to obtain L p ( Y i ) for each distinct haplotype. (1) L P ( Y i ) = ∑ j = 1 , … , H and X j = Y i L P ( X j )

In this reduced state space, we modify the Baum–Welch forward equations 30 to obtain L k ( . ) recursively for k = P + 1, P + 2, ...., Q . (2) L k + 1 ( Y i ) = [ [ 1 − θ k ] L k ( Y i ) + N i θ k H ∑ j = 1 , … , U L k ( Y i ) ] × P ( S k + 1 ∣ Y i )

In equation (2) , θ k denotes the template switch probability between markers k and k +1 (analogous to a recombination fraction), S k +1 is the genotype in the study sample, P ( S k +1 | Y i ) is the genotype emission probability, and N i is the number of haplotypes matching Y i in the original state space (for example, in Fig. 1 , N 1 = 4, N 2 = 2, and N 3 = 2). Once we obtain L Q ( . ) values for all the reduced states, we use them to calculate L Q ( X j ) at the final block boundary, enabling us to transition between blocks. To accomplish this, we split probability L Q ( . ) into two parts, L Q NR ( . ) and L Q R ( . ) , where L Q NR ( . ) denotes the left probability at marker Q when no template switches occur between markers P and Q and L Q R ( . ) denotes the probability when at least one switch occurs. This leads to equation (3) (where i is such that Y i = X j ) (3) L Q ( X j ) = L Q R ( Y i ) × [ 1 N i ] + L Q NR ( Y i ) [ L P ( X j ) L P ( Y i ) ]

Similar equations can be derived for the right probabilities R k (.) and R k ( . ) . Once we have the left and right probabilities for all the reduced states, the posterior probabilities for a template including any allele of interest at marker k can be calculated within the reduced state space as (4) P ( Y i ) = [ ∑ j = 1 , … , H abd X j = Y i L P ( X j ) R Q ( X j ) ] × [ L k NR ( Y i ) L P ( Y i ) × R k NR ( Y i ) R Q ( Y i ) ] + 1 N [ L k ( Y i ) R k ( Y i ) − L k NR ( Y i ) R k NR ( Y i ) ]

Methods that perform phasing and imputation simultaneously (for example, MaCH 29 and IMPUTE 31 ) have a computational cost proportional to the number of study samples ( N ), the number of genotyped markers in the reference panel ( M ), and the square of the number of reference haplotypes ( H 2 ), or in total O ( NMH 2 ). In the context of prephasing, as in minimac and IMPUTE2 (ref. 9 ), this computational cost is reduced to O ( NMH ).

For imputation using mimimac3, we break up a chromosome into K consecutive segments. If U i denotes the number of unique haplotypes and M i denotes the number of markers in segment i , then complexity is O ( N × ∑ i = 1 , … , K U i M i ) + O ( N K H ) The second term accounts for the complexity of transitions between blocks, which occur in the original state space. Thus, although very short segments could reduce the number of unique haplotypes per segment ( U i ) and complexity measured by the first term, such segments would also increase the total number of segments ( K ) and complexity measured by the second term. An optimal allocation of genomic regions must balance these two goals.

We implement a recursive dynamic programming algorithm to find the optimal allocation of the genomic segments, as a brute force approach is not feasible (~2 M –1 alternatives). We assume that the optimal complexity of imputation until marker i < M is denoted by C( i ) and calculate C ( M ) recursively as (5) C ( M ) = min i = 1 , 2 , … , M − 1 { C ( i ) + U ( i , M ) × ( M − i + 1 ) + 2 H }

In equation (5) , C ( i ) is the optimal cost for imputation from marker 1 to marker i and U ( i , M ) is the number of unique haplotypes between marker i and marker M (inclusive). This expression requires at most M 2 comparisons; this number can be further reduced because we do not need to consider large segments, as the unique number of haplotypes in large segments will be close to the total number of haplotypes.

We implemented both the expectation–maximization algorithm and Monte Carlo Markov chain (MCMC) sampling to estimate θ for adjacent marker pairs and ε for each marker. θ is the template switching rate, which reflects a combination of population recombination rates and relatedness between the samples. ε is the error parameter, which reflects a combination of genotyping error, gene conversion events, and recurrent mutation (for details, see refs. 9 , 29 ).

We evaluated the performance of minimac3 (v1.0.14) in comparison to the three most commonly used imputation tools: minimac2 (v2014.9.15), IMPUTE2 (v2.3.1), and Beagle 4.1 (v22Feb16) ( Table 1 ). We combined chromosome 20 data across multiple whole-genome sequencing studies to generate large reference panels. We compared results for the following seven reference panels: (i) 1000G Phase 1: 1,092 individuals from 1000 Genomes Project Phase 1 (refs. 25 , 26 ), (ii) AMD: 2,074 individuals sequenced for study of age-related macular degeneration 32 , (iii) 1000G Phase 3: 2,504 individuals from 1000 Genomes Project Phase 3 (ref. 1 ), (iv) SardiNIA: 3,489 individuals from the SardiNIA project 4 , (v) COMBINED: 9,341 individuals combined together from AMD, SARDINIA, the BRIDGE study of bipolar disorder (L.J.S., unpublished data) (2,464 samples), and the Minnesota Twins study 33 (1,314 samples), (vi) Mega: 11,845 individuals obtained by merging COMBINED and G1KP3, and (vii) HRC v1.1: 32,390 individuals from HRC 14 . To mimic a GWAS, we selected 25 unrelated individuals each from AMD, SardiNIA, BRIDGE Study, and Minnesota Twins and masked all variants except those typed on the Illumina Duo 1M chip (resulting in ~20,000 genotyped variants for chromosome 20). To evaluate imputation accuracy, we estimated the squared Pearson correlation coefficient ( r 2 ) between the imputed genotype probabilities and genotype calls from sequence data. We evaluated imputation accuracy at the 227,925 variants that were present in all the respective data sets and had MAF of at least 0.00005 in all contributing studies. For each of the combinations of the four imputation methods and seven reference panels, we recorded the average imputation accuracy, total computational time, and physical memory required to impute 100 GWAS individuals.

The Michigan Imputation Server implements the whole-genotype imputation workflow using the MapReduce programming model for efficient parallelization of computationally intensive tasks. We use the open source framework Hadoop to implement all workflow steps. Maintenance of the server, including node configuration (for example, amount of parallel tasks, memory for each chunk, and monitoring of all nodes), is achieved using the Cloudera Manager. During cluster initialization, reference panels, genetic maps, and software packages are distributed across all cluster nodes using the Hadoop file system HDFS. The imputation workflow itself consists of two steps: first, we divide the data into non-overlapping chunks (here, chromosome segments of 20 Mb). Second, we run an analysis (here, quality control or phasing and imputation) in parallel across chunks. To avoid edge effects, 5 Mb for phasing and 500 kb for imputation are added to each chunk. Finally, all results are combined to generate an aggregate final output.

Genotype imputation can be implemented with MapReduce, as the computationally expensive whole-genome calculations can be split into independent chromosome segments. Our imputation server accepts phased and unphased GWAS genotypes in VCF file format. File format checks and initial statistics (numbers of individuals and SNVs, detected chromosomes, unphased/phased data set, and number of chunks) are generated during the preprocessing step. Then, the submitted genotypes are compared to the reference panel to ensure that alleles, allele frequencies, strand orientation, and variant coding are correct. In this first MapReduce analysis, the map function calculates the VCF statistics for each file chunk, and the reducer summarizes the results and forwards only chunks that pass quality control to the subsequent imputation step ( Supplementary Fig. 2 ). The MapReduce imputation step constitutes a map-only job. This means that no reducer is applied and each mapper imputes genotypes using minimac3 on the previously generated chunk. If the user has uploaded unphased genotypes, the data are prephased with one of the available phasing engines: Eagle 2, HAPI-UR 34 , or SHAPEIT 17 . A post-processing step generates a zipped and indexed VCF file (using bgzip and tabix 35 ) for each imputed chromosome. To minimize the input/output load, the reference panel is distributed across available nodes in the cluster using the distributed cache feature of Hadoop. To ensure data security, imputation results are encrypted on the fly using a one-time password. All result files and reports can be viewed or downloaded via the web interface.

The imputation server workflow has been integrated into Cloudgene 24 to provide a graphical user interface. Cloudgene is a high-level workflow system for Apache Hadoop designed as a web application using Bootstrap, CanJs, and JQuery. On the server side, all necessary resources are implemented in Java using the RESTful web framework Restlet. The Cloudgene API provides methods for the execution and monitoring of MapReduce jobs and can be seen as an additional layer between Hadoop and the client. The imputation server is integrated into Cloudgene using the provided workflow definition language and its plugin interface. On the basis of the workflow information, Cloudgene automatically renders a web form for all required parameters to submit individual jobs to the Cloudgene server. The server communicates and interacts with the Hadoop cluster and receives feedback from currently executing jobs. Client and server communicate by asynchronous HTTP requests (AJAX) with JSON as an interchange format. All transmissions between server and client are encrypted using SSL (Secure Socket Layer).

Parameter estimates for the reference panel can be precalculated and saved to speed up the imputation process. To examine the importance of GWAS panel individuals during the parameter estimation step, we used 938 unrelated individuals from 53 worldwide populations from the Human Genome Diversity Panel 36 . We compared the imputation accuracy across three parameter estimation methods: constant parameters ( θ = 0.001 and ε = 0.01), reference panel only for updating the parameters using a leave-one-out method, and reference and GWAS panels for updating. The results of imputation accuracy evaluated on ~6,000 masked variants from chromosomes 20–22 on the ExomeArray are shown in Supplementary Figure 3 . We see that updating the parameters results in increased imputation accuracy in comparison to constant estimates (especially for European samples, where imputation r 2 increases from 0.35 to 0.45 in the lowest MAF bin). However, including the target panel (along with the reference panel) typically produced only a very small improvement in imputation accuracy.

The idea of state space reduction can be applied not only to improve HMM implementation efficiency but also to store large reference panels using less disk space. We introduce the m3vcf (minimac3 VCF) format, which is compatible with the Variant-Call Format (VCF) format. m3vcf files save each genomic segment in series where each segment has the list of bi- and multiallelic variants in order along with the unique haplotypes at these variants and a single line at the beginning of the block that describes which individual maps to which unique haplotype. This format reduces disk space requirements because it saves only the unique haplotypes at each block rather than all the haplotypes. The way in which the unique haplotypes are ordered (along columns) creates long runs of 0's and 1's (as they are ordered lexicographically from the first variant to the last variant) and is thus even more helpful in disk space reduction when using standard file compression methods such as gzip.

We calculated the order of disk space saved using m3vcf files in comparison to the usual VCF files (in both unzipped and zipped formats) and found that, for 1000 Genomes Project Phase 1 with ~1,000 reference samples, we save 60% of disk space using zipped m3vcf files in comparison to zipped VCF files and 93% when compared across unzipped formats. The saving is even greater for larger panels. For example, for the HRC reference panel with ~33,000 samples, we save ~84% and 98% of disk space using zipped and unzipped m3vcf files, respectively ( Supplementary Table 4 ).